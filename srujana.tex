\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

% %%%%%%%%% TITLE
% \title{\LaTeX\ Author Guidelines for CVPR Proceedings}


% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
%\thispagestyle{empty}

% %%%%%%%%% ABSTRACT
% \begin{abstract}
%   The ABSTRACT is to be in fully-justified italicized text, at the top
%   of the left-hand column, below the author and affiliation
%   information. Use the word ``Abstract'' as the title, in 12-point
%   Times, boldface type, centered relative to the column, initially
%   capitalized. The abstract is to be in 10-point, single-spaced type.
%   Leave two blank lines after the Abstract, then begin the main text.
%   Look at previous CVPR abstracts to get a feel for style and length.
% \end{abstract}


%%%%%%%%% BODY TEXT
\section{Entropy rates of a Stochastic Process}


A stochastic process, also called a random process, is a set of random variables that model a non deterministic system.If the random variables are dependent or in particular, if the random variables form a stationary process, we will show, just as in the i.i.d.case, that the entropy
\begin{math}H (X 1 , X 2 , \dots , X n )\end{math} grows (asymptotically) linearly with n at a rate \begin{math}H ( X )\end{math}, which we will call the entropy rate of the process.

An indexed sequence of random variables is called a stochastic process \begin{math}
{X_i}\end{math}. 
There can be arbitrary dependencies among random variables in general. The joint probability mass functions characterise the process.\\
\begin{math}
Pr\{(X_1 , X_2 , \dots , X_n )=(x_1 , x_2 , \dots, x_n )\} = p(x_1 , x_2 , \dots , x_n ), (x_1 , x_2 , \dots ,x_n ) \in X_n \quad for \quad n = 1, 2, \dots
\end{math} \\
\\
\textbf{Definition} A stochastic process is said to be stationary if the joint
distribution of any subset of the sequence of random variables is invariant
with respect to shifts in the time index; that is,\\
\begin{mathbf} \\
Pr\{X_1 = x_1 , . . . , X_n = x_n \}
= Pr\{X_1+l = x_1 , . . . , X_n+l = x_n \}
\end{mathbf} \\
\\ for every n and every shift l and \forall x_1 , x_2 , \dots , x_n \in X .
\subsection{Markov chains}

An example of a stochastic process with dependency is one in which each random variable is conditionally independent of all the other previous random variables.
Markov is the name for such a process.\\
\\
\textbf{Definition} A discrete stochastic process \begin{math}
X_1 , X_2 , \dots 
\end{math} is said to be a
Markov chain if for n = 1, 2, \dots , \\
\\
\begin{mathbf}
Pr(X_{n+1} = x_{n+1} |X_n = x_n , X_{n-1} = x_{n-1} , . . . , X_1 = x_1 )
= Pr (X_{n+1} = x_{n+1} |X_n = x_n )
\end{mathbf}\\
\\ \begin{math}
\quad \forall \quad x_1 , x_2 , . . . , x_n , x_{n+1} \in X .
\end{math}\\ 
\\
In this case, the joint probability mass function of the random variables
can be written as\\
\\ \begin{math}
p(x_1 , x_2 , \dots, x_n ) = p(x_1 )p(x_2 |x_1 )p(x_3 |x_2 ) · · · p(x_n |x_{n-1} ).
\end{math}\\
\\ It is assumed that the Markov chains are time invariant unless otherwise
stated. \\
\\ \textbf{Definition} The Markov chain is said to be time invariant if the conditional probability \begin{math} p(x_n+1 |x_n )\end{math} does not depend on n; that is, for n = 1, 2, \dots , 
\begin{eqnarray*}
\\ Pr\{X_{n+1} = b|X_n = a\} = Pr\{X_2 = b|X_1 = a\} 
\end{eqnarray*} \begin{math} \forall \quad  a, b\in X .\end{math} \\
\\
\textbf{State and Transition matrix:} \\ If \begin{math} {X_i} \end{math} is a Markov chain, \begin{math} X_n \end{math} is called the state at time n. A time-invariant Markov chain is characterized by its initial state and a probability transition matrix, given by
\begin{math}
 P = [P_{ij} ], \quad for \quad i, j \in {1, 2, \dots , m}, 
\end{math}
\\ where \begin{math} P_ij = Pr\{X_{n+1} = j |X_n = i\}. \end{math} \\
\\
If it is possible to go with positive probability from any state of the
Markov chain to any other state in a ﬁnite number of steps, the Markov
chain is said to be \textbf{irreducible}. \\If the largest common factor of the lengths
of different paths from a state to itself is 1, the Markov chain is said to
\textbf{aperiodic}.\\
\\ 
\textbf{Example: }Consider a two-state Markov chain with a probability transition matrix given by: 
 \begin{eqnarray*}
\begin{bmatrix}
{1 - \alpha} & \alpha \\
\beta & 1 - \beta
\end{bmatrix}
\end{eqnarray*}
\\
Let's say the stationary distribution is represented as a vector \begin{math}
 \mu \end{math} with the stationary probabilities of states 1 and 2 as its components. Then, by solving the equation \begin{math}
  \mu P = \mu \end{math} or, more simply, by balancing probabilities, the stationary probability can be obtained. 
  \begin{equation*}
      \mu P = \mu \implies \mu P = \mu I \implies \mu(P - I) = 0 
  \end{equation*}
  \\The net probability flow across any cut set in the state transition graph is zero for the stationary distribution. \\ As \begin{math}
   \mu \end{math} is a probability distribution it follows that \begin{math} \mu_1 + \mu_2 = 1. \end{math} \\ Solving for \begin{math} \mu\end{math}, using the above and the following equation, \quad
\begin{math}
      \mu_1\alpha = \mu_2\beta.
\end{math} The stationary distribution is given by:
\begin{equation*}
    \mu_1 = \frac{\beta}{\alpha + \beta}, \: \mu_2 = \frac{\alpha}{\alpha + \beta}
\end{equation*} 
The resulting process will be stationary if the Markov chain has an initial state drawn according to the stationary distribution. The entropy of the state \begin{math} X_n
\end{math} is: \\
\begin{equation*}
    H(X_n) = H( \frac{\beta}{\alpha + \beta}, \frac{\alpha}{\alpha + \beta} ).
\end{equation*}\\
\subsection{Entropy Rate}
Since a stochastic process defined by a Markov chain that is irreducible, aperiodic and positive recurrent has a stationary distribution, the entropy rate is independent of the initial distribution. is the asymptotic distribution of the chain.\\
\\ \textbf{Definition: }The entropy of a stochastic process \begin{math} {X i } \end{math} is deﬁned by
\begin{equation*}
    H(X) = \lim_{n \to \infty} {\frac{1}{n}}H(X_1,H_2, \dots H_3)
\end{equation*} when the limit exists.\\
\\We can also deﬁne a related quantity for entropy rate:
\begin{equation*}
    H'( X ) = \lim_{n \to \infty} H (X_n |X_{n-1} , X_{n-2} , . . . , X_1 )
\end{equation*} when the limit exists.\\
\\\begin{math}
      H(X) 
\end{math} and \begin{math}
      H'(X)
\end{math} are two different notions of entropy rate. The first is the entropy of the n random variables per symbol, and the second is the conditional entropy of the last random variable given the past. \\ We now show that both limits exist and are identical for stationary processes.\\
\\ \textbf{Entropy of a stationary Markov Chain:}\\
For a stationary Markov chain, the entropy rate is given by
\begin{equation*}
    H ( X ) = H'( X ) = \lim H (X_n |X_{n-1} , . . . , X_1 ) 
\end{equation*}
\begin{equation}
= \lim H (X_n |X_{n-1} )
= H (X_2 |X_1 )
\end{equation}\\
\textbf{Theorem: }Let \begin{math}
      \{X_i\}
\end{math} be a stationary Markov chain with stationary distribution µ and transition matrix P . \\Let \begin{math}
      X_1  \char`\~ \mu
\end{math} Then the entropy rate is
\begin{equation*}
    H(X) = -\sum_{ij}\mu_i P_{ij}\mul logP_{ij}
\end{equation*}
\textbf{Proof}
\begin{equation*}
    H ( X ) = H (X_2 |X_1 ) 
\end{equation*}
\begin{equation*}
   = \sum_i \mu_i(\sum_j -P_{ij}logP_{ij})
\end{equation*}
%-------------------------------------------------------------------------
\subsection{Functions of Markov chains}
Let
\begin{math} X 1 , X 2 , \dots , X n , \dots \end{math} be a stationary Markov chain, and let \begin{math} Y_i = \varphi (X_i)\end{math} be a process each term of which is a function of the corresponding state in the Markov chain. To compute \begin{math}
H ( Y ) \end{math}, we might compute \begin{math}
H (Y_n |Y_{n-1} , . . . , Y_1 ) \end{math} for each n and ﬁnd the limit.
Upper and lower bounds converging to the limit from above and below might be advantageous computationally. When the difference between the upper and lower bounds is small, we can stop the computation and get a solid estimate of the limit.\\ We know that  \begin{math}  H (Y_n |Y_{n-1} , . . . , Y_2 , Y_1 )  \end{math} converges to \begin{math} H(Y) \end{math} i.e, \begin{math} H (Y_n |Y_{n-1} , . . . , Y_2 , Y_1 )  \leq H(Y) \end{math}\\
\\The lemma shows that the interval between the upper and the lower bounds decreases in length.\\
\\ \textbf{Lemma}\\
\begin{equation*}
    H (Y_n |Y_{n-1} , . . . , Y_1 ) - H (Y_n |Y_{n-1} , . . . , Y_1 , X_1 ) \to 0.
\end{equation*}
\\ \textbf{Proof}\\
\\The LHS can also be written as:
\begin{equation*}
     H (Y_n |Y_{n-1} , . . . , Y_1 ) - H (Y_n |Y_{n-1} , . . . , Y_1 , X_1 )
\end{equation*}
\begin{equation*}
    = I (X_1 ; Y_n |Y_{n-1}, . . . , Y_1 )
\end{equation*}
This mutual information is always less than or equal to \begin{math}
H(X_1) \end{math} i.e, 
\begin{equation*}
    I (X_1 ; Y_1 ,Y_2 , . . . , Y_n  ) \leq H(X_1)
\end{equation*} 
As n tends to infinity, \begin{math}\lim I (X_1 ; Y_1 ,Y_2 , . . . , Y_n ) \end{math}
exists.\\ From the chain rule, 
\begin{equation*}
    H (X) \geq \lim_{n \to \infty} I (X_1 ; Y_1 , Y_2 , . . . , Y_n )
\end{equation*}
\begin{equation*}
    = \lim_{n \to \infty} \sum_{i=1}^{n} I (X_1 ; Y_i |Y_{i-1} , . . . , Y_1 )
\end{equation*}
\begin{equation*}
    = \sum_{i=1}^{\infty} I (X_1 ; Y_i |Y_{i-1} , . . . , Y_1 )
\end{equation*}
As n tends to infinity, this sum of infinite terms is ﬁnite and the terms are non-negative, the terms must tend to 0; which proves the lemma, that is,
\begin{equation*}
    \lim I (X_1 ; Y_n |Y_{n-1} , . . . , Y_1 ) = 0
\end{equation*}
From this lemma, we have the following theorem:\\
\\ \textbf{Theorem:}\\
If \begin{math} X_1 , X_2 , . . . , X_n \end{math} form a stationary Markov chain, and
\begin{math} Y_i = \varphi(X_i ) \end{math}, then 
\begin{equation*}
    H (Y_n |Y_{n-1} , . . . , Y_1 , X_1 ) \leq H ( Y ) \leq H (Y_n |Y_{n-1} , . . . , Y_1 )
\end{equation*}
and
\begin{equation*}
   \lim H (Y_n |Y_{n-1} , . . . , Y_1 , X_1 ) = H ( Y ) 
\end{equation*}
similarly, 
\begin{equation*}
    \lim H (Y_n |Y_{n-1} , . . . , Y_1 ) = H(Y)
\end{equation*}
\begin{math}

\end{math}
% \subsection{Mathematics}

% Please number all of your sections and displayed equations.  It is
% important for readers to be able to refer to any particular equation.  Just
% because you didn't refer to it in the text doesn't mean some future reader
% might not need to refer to it.  It is cumbersome to have to use
% circumlocutions like ``the equation second from the top of page 3 column
% 1''.  (Note that the ruler will not be present in the final copy, so is not
% an alternative to equation numbers).  All authors will benefit from reading
% Mermin's description of how to write mathematics:



\end{document}
